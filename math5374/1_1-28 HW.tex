% -*- coding:utf-8 -*-
% LATEX PREAMBLE --- needs to be imported manually
\documentclass[12pt]{article}
% \special{papersize=3in,5in}

\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath,amsthm}
\pagestyle{empty}
\setlength{\parindent}{0in}

\newcommand{\detail}[1]{{\LARGE#1\par}~}
\newcommand{\refs}[1]{{\LARGE\textit{References: }#1\par}\hfill.}
\newcommand*{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\lgmth}[1]{\begingroup\LARGE\[#1\]\endgroup}

%%% commands that do not need to imported into Anki:
\usepackage{mdframed}
\newcommand*{\tags}[1]{\paragraph{tags: }#1\bigskip}
\newcommand*{\xfield}[1]{\begin{mdframed}[font=\sffamily\LARGE]\centering #1\end{mdframed}\bigskip}
\newenvironment{field}{}{}
\newcommand*{\xplain}[1]{\begin{mdframed}\texttt{#1}\end{mdframed}\bigskip}
\newenvironment{plain}{\ttfamily}{\par}
\newenvironment{note}{}{}
% END OF THE PREAMBLE

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}} % |Irrationals 

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\dta}{\mt{\delta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->
\newcommand{\lba}{ \mt{\longmapsto} }     % element maps to |--->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}
\newcommand{\abk}[1]{\mt{\langle}#1\mt{\rangle}}

\newcommand{\ps}{\mt{+} }
\newcommand{\ms}{\mt{-} }

\newcommand{\ls}{\mt{<} }
\newcommand{\gr}{\mt{>} }

\newcommand{\lse}{\mt{\leq} }
\newcommand{\gre}{\mt{\geq} }

\newcommand{\eql}{\mt{=} }

\newcommand{\pr}{\mt{^\prime} } 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\infy}{\mt{\infty} }
\newcommand{\unn}{\mt{\cup} }
\newcommand{\inn}{\mt{\cap} }
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\rln}{ \mt{\sim} }
\newcommand{\dvd}{ \mt{\vert} }
\newcommand{\ndvd}{ \mt{\not\vert} }
\newcommand{\eqw}{ \mt{ \equiv } }
\newcommand{\lcg}{ \mt{\gamma} }

\newcommand{\edp}{\mt{\bigoplus} }

\newcommand{\wit}[1]{\mt{\widetilde{#1}}}

\newcommand{\mxc}[5]{ % Matrix Column: entry entry entry entry DIMENSION
\underset{#5 \times 1}{
  \begin{bmatrix}
     #1 \\
     #2 \\
     #3 \\
     #4
  \end{bmatrix}
  }
}

\newcommand{\mxr}[5]{ % Matrix Row:    entry entry entry entry DIMENSION
\underset{1 \times #5}{
  \begin{bmatrix}
     #1 & #2 & #3 & #4
  \end{bmatrix}
  }
}

\begin{document}

2.1

\

Show that if a matrix A is both triangular and unitary, then it is diagonal.

\

Let A \mem \uf{\bc}{m \times n} be triangular and unitary. We wish to show that A is, therefore, diagonal.

However, suppose A is also not diagonal.

Then, \exs \uw{a}{j} \mem A and some \uw{a}{i, j} \mem \uw{a}{j} such that \uw{a}{i, j} $\neq$ 0 and i $\neq$ j.

Without loss of generality, let's assume A is upper triangular (i.e. that i \ls j).

Notice: 

\uw{a}{i, i} \mem \uw{a}{i} and \uw{a}{i, i} $\neq$ 0 (since A is triangular).

Since A is unitary, \uw{a}{i}* $\cdot$ \uw{a}{j} \eql 0. Specifically,

\begin{displaymath}
  a_{1, i}* \cdot a_{1, j} + a_{2, i}* \cdot a_{2, j} + ... + a_{i, i}* \cdot a_{i, j} + ... + a_{j, i}* \cdot a_{j, j} +  ... a_{m, i}* \cdot a_{m, j} \eql 0
\end{displaymath}

Notice: \uw{a}{i, i} $\cdot$ \uw{a}{i, j} $\neq$ 0 and \uw{a}{k, i} $\cdot$ \uw{a}{k, j} \eql 0 for k \gr i. So,

\begin{displaymath}
  a_{i, i}* \cdot a_{i, j} + ... + a_{j, i}* \cdot a_{j, j} +  ... a_{m, i}* \cdot a_{m, j} \eql 0
\end{displaymath}

2.3

\

Let A \mem \uf{\bc}{m \times m} be hermitian. An eigenvector of A is a nonzero vector x \mem \uf{\bc}{m} such that Ax \eql $\lambda$x for some $\lambda$ \mem \bc, the corresponding eigenvalue.

\begin{enumerate}
  \item Prove that all eigenvalues of A are real.

Let $\lambda$ be the eigenvector of x such that Ax \eql $\lambda$x. Notice:

\begin{displaymath}
  \splt{ Ax & = \lambda x \\
  (Ax)^* & = (\lambda x)^* \\
  x^* A^* & = x^*\lambda^* \\
  x^* A^* x & = x^*\lambda^* x \\
  x^* A x & = x^*\lambda^* x \\
  x^* A x & = \lambda^* x^* x \\
  x^* A x & = \lambda^* \|x\|^2 \\
  \frac{x^* A x}{\|x\|^2} & = \lambda^* 
  }
\end{displaymath}

and

\begin{displaymath}
  \splt{ Ax & = \lambda x \\
  x^* Ax & = x^* \lambda x \\
  & = \lambda x^* x \\
  & = \lambda \|x\|^2 \\
  \frac{x^* Ax }{\|x\|^2} & = \lambda
  }
\end{displaymath}

Hence, \uf{$\lambda$}{*} \eql $\lambda$

  \item Prove that if x and y are eigenvectors corresponding to distinct eigenvalues, then x and y are orthogonal.
 
Let v1 and v2 be eigenvectors of A with corresponding eigenvalues $\lambda_1$ and $\lambda_2$ such that $\lambda_1 \neq \lambda_2$. So,

\begin{displaymath}
  Av_1 \eql \lambda_1v_1 \tab Av_2 \eql \lambda_2 v_2
\end{displaymath}

Notice:

\begin{displaymath}
  \splt{ Av_2 & = \lambda_2 v_2 \\
  v_1^* Av_2 & = v_1^* \lambda_2 v_2 \\
  & = \lambda_2 v_1^* v_2 \\
  }
\end{displaymath}

and

\begin{displaymath}
  \splt{ Av_2 & = \lambda_2 v_2 \\
  v_1^* Av_2 & = v_1^* A v_2 \\
  & = ((v_1^* A v_2)^*)^* \\
  & = (v_2^* A^* v_1)^* \\
  & = (v_2^* A v_1)^* \\
  & = (v_2^* \lambda_1 v_1)^* \\
  & = v_1^* \lambda_1^* v_2 \\
  & = \lambda_1 v_1^* v_2 \\
  }
\end{displaymath}

Since 

\begin{displaymath}
  \lambda_2 v_1^* v_2 = \lambda_1 v_1^* v_2
\end{displaymath}

and $\lambda_1 \neq \lambda_2$, v1 and v2 must be orthogonal.

\end{enumerate}

3.2

\

Let $\|.\|$ denote a norm on \uf{\bc}{m} and a matrix norm on A. Show that p(A) \lse $\|A\|$, where p(A) is the spectral radius of A (i.e. the largest absolute value of an eigenvalue of A).

\

Recall the definition of a induced matrix norm:

\begin{displaymath}
  \|A\|_{(m, n)} = \underset{x \in \bc^m, x \neq 0} \sup \frac{\|Ax\|_{(m)}}{\|x\|_{(n)}} = \underset{x \in \bc^m, \|x\|_{(m)} = 1} \sup \|Ax\|_{(m)}
\end{displaymath}

Let v be the eigenvector of A corresponding to the eigenvalue $\lambda$ of largest magnitude (i.e. the eigenvalue with the largest absolute value).

By definition,

\begin{displaymath}
  \|A\|_{(m, n)} = \underset{x \in \bc^m, x \neq 0} \sup \frac{\|Ax\|_{(m)}}{\|x\|_{(n)}}
\end{displaymath}

And since eigenvectors of a are members of \uf{\bc}{m},

\begin{displaymath}
  \splt{
  \underset{x \in \bc^m, x \neq 0} \sup \frac{\|Ax\|_{(m)}}{\|x\|_{(m)}} & \gre \underset{x \in \bc^m, x \neq 0} \sup \frac{\|\lambda x\|_{(m)}}{\|x\|_{(m)}} \\
  & = \underset{x \in \bc^m, x \neq 0} \sup \frac{|\lambda| \| x\|_{(m)}}{\|x\|_{(m)}} \\
  & = \underset{x \in \bc^m, x \neq 0} \sup |\lambda| \\
  & = |\lambda| \\ 
  & = p(A)
  }
\end{displaymath}

Hence, p(A) \lse $\|A\|$


\end{document}