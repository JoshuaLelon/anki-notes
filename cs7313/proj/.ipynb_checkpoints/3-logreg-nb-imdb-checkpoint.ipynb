{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Movie Reviews (using Naive Bayes, Logistic Regression, and Ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to cover Naive Bayes, Logistic regression, and ngrams (some pretty classic techniques!) for sentiment classification.  We will be using sklearn and the fastai library.\n",
    "\n",
    "In a future lesson, we will tackle this same problem of sentiment classification using deep learning, so that you can compare the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content here was extended from [Lesson 10 of the fast.ai Machine Learning course](https://course.fast.ai/lessonsml1/lesson10.html). Linear model is pretty close to the state of the art here.  Jeremy surpassed state of the art using a RNN in fall 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The fastai library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will begin using [the fastai library](https://docs.fast.ai) (version 1.0) in this notebook.  We will use it more once we move on to neural networks.\n",
    "\n",
    "The fastai library is built on top of PyTorch and encodes many state-of-the-art best practices. It is used in production at a number of companies.  You can read more about it here:\n",
    "\n",
    "- [Fast.ai's software could radically democratize AI](https://www.zdnet.com/article/fast-ais-new-software-could-radically-democratize-ai/) (ZDNet)\n",
    "\n",
    "- [fastai v1 for PyTorch: Fast and accurate neural nets using modern best practices](https://www.fast.ai/2018/10/02/fastai-ai/) (fast.ai)\n",
    "\n",
    "- [fastai docs](https://docs.fast.ai/)\n",
    "\n",
    "### Installation\n",
    "\n",
    "With conda:\n",
    "\n",
    "`conda install -c pytorch -c fastai fastai=1.0`\n",
    "\n",
    "Or with pip:\n",
    "\n",
    "`pip install fastai==1.0`\n",
    "\n",
    "More [installation information here](https://github.com/fastai/fastai/blob/master/README.md).\n",
    "\n",
    "Beginning in lesson 4, we will be using GPUs, so if you want, you could switch to a [cloud option](https://course.fast.ai/#using-a-gpu) now to setup fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB, We will use the version hosted as part [fast.ai datasets](https://course.fast.ai/datasets.html) on AWS Open Datasets. \n",
    "\n",
    "The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n",
    "\n",
    "The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sklearn_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and term document matrix creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast.ai has a number of [datasets hosted via AWS Open Datasets](https://course.fast.ai/datasets.html) for easy download. We can see them by checking the docs for URLs (remember `??` is a helpful command):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object ` URLs` not found.\n"
     ]
    }
   ],
   "source": [
    "?? URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to start working on a sample of your data before you use the full dataset-- this allows for quicker computations as you debug and get your code working. For IMDB, there is a sample dataset already available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/racheltho/.fastai/data/imdb_sample')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to use this dataframe, but are just loading it to get a sense of what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  is_valid\n",
       "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1  positive  This is a extremely well-made film. The acting...     False\n",
       "2  negative  Every once in a long while a movie will come a...     False\n",
       "3  positive  Name just says it all. I watched this movie wi...     False\n",
       "4  negative  This movie succeeds at being one of the most u...     False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [TextList](https://docs.fast.ai/text.data.html#TextList) from the fastai library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                         .split_from_df(col=2)\n",
    "                         .label_from_df(cols=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring what our data looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good first step for any data problem is to explore the data and get a sense of what it looks like.  In this case we are looking at movie reviews, which have been labeled as \"positive\" or \"negative\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that a non-word maps to xxunk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we covered in the last lesson, a term-document matrix represents a document as a \"bag of words\", that is, we don't keep track of the order the words are in, just which words occur (and how often)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we used [sklearn's CountVectorizer](https://github.com/scikit-learn/scikit-learn/blob/55bf5d9/sklearn/feature_extraction/text.py#L940).  Today we will create our own (similar) version.  This is for two reasons:\n",
    "- to understand what sklearn is doing underneath the hood\n",
    "- to create something that will work with a fastai TextList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our term-document matrix, we first need to learn about **counters** and **sparse matrices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counters are a useful Python object.  If you aren't familar with them, here is how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter([4,2,8,8,4,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 2, 2: 1, 8: 3})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 1, 3])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([4, 2, 8])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counters are from the collections module (along with OrderedDict, defaultdict, deque, and namedtuple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrices (in Scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we've reduced over 19,000 words down to 6,000, that is still a lot! Most tokens don't appear in most reviews.  We want to take advantage of this by storing our data as a **sparse matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix with lots of zeros is called **sparse** (the opposite of sparse is **dense**).  For sparse matrices, you can save a lot of memory by only storing the non-zero values.\n",
    "\n",
    "<img src=\"images/sparse.png\" alt=\"floating point\" style=\"width: 30%\"/>\n",
    "\n",
    "Another example of a large, sparse matrix:\n",
    "\n",
    "<img src=\"images/Finite_element_sparse_matrix.png\" alt=\"floating point\" style=\"width: 30%\"/>\n",
    "\n",
    "[Source](https://commons.wikimedia.org/w/index.php?curid=2245335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the most common sparse storage formats:\n",
    "- coordinate-wise (scipy calls COO)\n",
    "- compressed sparse row (CSR)\n",
    "- compressed sparse column (CSC)\n",
    "\n",
    "Let's walk through [these examples](http://www.mathcs.emory.edu/~cheung/Courses/561/Syllabus/3-C/sparse.html)\n",
    "\n",
    "There are actually [many more formats](http://www.cs.colostate.edu/~mcrob/toolbox/c++/sparseMatrix/sparse_matrix_compression.html) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class of matrices (e.g, diagonal) is generally called sparse if the number of non-zero elements is proportional to the number of rows (or columns) instead of being proportional to the product rows x columns.\n",
    "\n",
    "**Scipy Implementation**\n",
    "\n",
    "From the [Scipy Sparse Matrix Documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html)\n",
    "\n",
    "- To construct a matrix efficiently, use either dok_matrix or lil_matrix. The lil_matrix class supports basic slicing and fancy indexing with a similar syntax to NumPy arrays. As illustrated below, the COO format may also be used to efficiently construct matrices\n",
    "- To perform manipulations such as multiplication or inversion, first convert the matrix to either CSC or CSR format.\n",
    "- All conversions among the CSR, CSC, and COO formats are efficient, linear-time operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our version of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 1,\n",
       "         5: 32,\n",
       "         21: 3,\n",
       "         71: 1,\n",
       "         189: 1,\n",
       "         748: 1,\n",
       "         289: 1,\n",
       "         285: 1,\n",
       "         62: 2,\n",
       "         221: 1,\n",
       "         666: 2,\n",
       "         59: 1,\n",
       "         13: 4,\n",
       "         2707: 1,\n",
       "         14: 6,\n",
       "         2877: 1,\n",
       "         11: 10,\n",
       "         18: 2,\n",
       "         358: 1,\n",
       "         0: 32,\n",
       "         77: 1,\n",
       "         15: 6,\n",
       "         478: 1,\n",
       "         1833: 1,\n",
       "         50: 3,\n",
       "         9: 10,\n",
       "         319: 1,\n",
       "         6: 1,\n",
       "         2745: 1,\n",
       "         12: 1,\n",
       "         115: 1,\n",
       "         4129: 1,\n",
       "         197: 2,\n",
       "         1331: 1,\n",
       "         25: 2,\n",
       "         324: 1,\n",
       "         10: 7,\n",
       "         3963: 1,\n",
       "         16: 4,\n",
       "         74: 1,\n",
       "         24: 3,\n",
       "         2819: 1,\n",
       "         5823: 1,\n",
       "         2597: 1,\n",
       "         710: 1,\n",
       "         3430: 1,\n",
       "         84: 1,\n",
       "         149: 1,\n",
       "         20: 1,\n",
       "         26: 1,\n",
       "         605: 1,\n",
       "         378: 1,\n",
       "         1057: 1,\n",
       "         251: 1,\n",
       "         258: 1,\n",
       "         1346: 1,\n",
       "         194: 1,\n",
       "         239: 1,\n",
       "         49: 1,\n",
       "         2766: 1,\n",
       "         1335: 1,\n",
       "         409: 1,\n",
       "         27: 3,\n",
       "         45: 1,\n",
       "         594: 1,\n",
       "         850: 1,\n",
       "         109: 1,\n",
       "         2603: 1,\n",
       "         430: 1,\n",
       "         1902: 1,\n",
       "         541: 1,\n",
       "         54: 2,\n",
       "         1107: 1,\n",
       "         608: 1,\n",
       "         404: 1,\n",
       "         736: 1,\n",
       "         44: 1,\n",
       "         204: 1,\n",
       "         23: 1,\n",
       "         3481: 1,\n",
       "         4739: 1,\n",
       "         456: 1,\n",
       "         4053: 1,\n",
       "         2421: 1,\n",
       "         30: 1,\n",
       "         337: 1,\n",
       "         967: 1,\n",
       "         58: 1,\n",
       "         207: 1,\n",
       "         2110: 1,\n",
       "         571: 1,\n",
       "         5037: 1,\n",
       "         579: 1,\n",
       "         1843: 1,\n",
       "         52: 1})"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter((movie_reviews.valid.x)[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxup'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.vocab.itos[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movie_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1fa92856acc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'movie_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "(movie_reviews.valid.x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \n",
       " \n",
       "  xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \n",
       " \n",
       "  xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \n",
       " \n",
       "  xxmaj welcome to xxmaj xxunk !"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movie_reviews.valid.x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_doc_matrix(label_list, vocab_len):\n",
    "    j_indices = []\n",
    "    indptr = []\n",
    "    values = []\n",
    "    indptr.append(0)\n",
    "\n",
    "    for i, doc in enumerate(label_list):\n",
    "        feature_counter = Counter(doc.data)\n",
    "        j_indices.extend(feature_counter.keys())\n",
    "        values.extend(feature_counter.values())\n",
    "        indptr.append(len(j_indices))\n",
    "        \n",
    "#     return (values, j_indices, indptr)\n",
    "\n",
    "    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
    "                                   shape=(len(indptr) - 1, vocab_len),\n",
    "                                   dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 0 ns, total: 56 ms\n",
      "Wall time: 54.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_term_doc = get_term_doc_matrix(movie_reviews.valid.x, len(movie_reviews.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 212 ms, sys: 4 ms, total: 216 ms\n",
      "Wall time: 210 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_term_doc = get_term_doc_matrix(movie_reviews.train.x, len(movie_reviews.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 6010)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[:,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6010)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_term_doc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could convert our sparse matrix to a dense matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sollett']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.vocab.itos[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[32,  0,  1,  0, ...,  1,  0,  0, 10],\n",
       "        [ 9,  0,  1,  0, ...,  1,  0,  0,  7],\n",
       "        [ 6,  0,  1,  0, ...,  0,  0,  0, 12],\n",
       "        [78,  0,  1,  0, ...,  0,  0,  0, 44],\n",
       "        ...,\n",
       "        [ 8,  0,  1,  0, ...,  0,  0,  0,  8],\n",
       "        [43,  0,  1,  0, ...,  8,  1,  0, 25],\n",
       "        [ 7,  0,  1,  0, ...,  1,  0,  0,  9],\n",
       "        [19,  0,  1,  0, ...,  2,  0,  0,  5]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_term_doc.todense()[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxeos'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.vocab.itos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos i saw this movie once as a kid on the late - late show and fell in love with it . \n",
       " \n",
       "  xxmaj it took 30 + years , but i recently did find it on xxup dvd - it was n't cheap , either - in a xxunk that xxunk in war movies . xxmaj we watched it last night for the first time . xxmaj the audio was good , however it was grainy and had the trailers between xxunk . xxmaj even so , it was better than i remembered it . i was also impressed at how true it was to the play . \n",
       " \n",
       "  xxmaj the xxunk is around here xxunk . xxmaj if you 're xxunk in finding it , fire me a xxunk and i 'll see if i can get you the xxunk . xxunk"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = movie_reviews.valid.x[1]; review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Since the word \"late\" shows up twice in this review (\"...as a kid on the late - late show...\"), confirm that a value of 2 is stored in the term-document matrix, for the row corresponding to this review and the column corresponding to the word \"late\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: Confirm this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200x6010 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 27848 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6010 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 81 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_term_doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_term_doc[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review has 81 distinct tokens in it, and 144 tokens total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  19, 248,  21, ...,   9,   0,  10,   0])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How could you convert review.data back to text (without just using review.text)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbos',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'once',\n",
       " 'as',\n",
       " 'a',\n",
       " 'kid',\n",
       " 'on',\n",
       " 'the',\n",
       " 'late',\n",
       " '-',\n",
       " 'late',\n",
       " 'show',\n",
       " 'and',\n",
       " 'fell',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'it',\n",
       " '.',\n",
       " '\\n \\n ',\n",
       " 'xxmaj',\n",
       " 'it',\n",
       " 'took',\n",
       " '30',\n",
       " '+',\n",
       " 'years',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'recently',\n",
       " 'did',\n",
       " 'find',\n",
       " 'it',\n",
       " 'on',\n",
       " 'xxup',\n",
       " 'dvd',\n",
       " '-',\n",
       " 'it',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'cheap',\n",
       " ',',\n",
       " 'either',\n",
       " '-',\n",
       " 'in',\n",
       " 'a',\n",
       " 'xxunk',\n",
       " 'that',\n",
       " 'xxunk',\n",
       " 'in',\n",
       " 'war',\n",
       " 'movies',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'we',\n",
       " 'watched',\n",
       " 'it',\n",
       " 'last',\n",
       " 'night',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'audio',\n",
       " 'was',\n",
       " 'good',\n",
       " ',',\n",
       " 'however',\n",
       " 'it',\n",
       " 'was',\n",
       " 'grainy',\n",
       " 'and',\n",
       " 'had',\n",
       " 'the',\n",
       " 'trailers',\n",
       " 'between',\n",
       " 'xxunk',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'even',\n",
       " 'so',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'better',\n",
       " 'than',\n",
       " 'i',\n",
       " 'remembered',\n",
       " 'it',\n",
       " '.',\n",
       " 'i',\n",
       " 'was',\n",
       " 'also',\n",
       " 'impressed',\n",
       " 'at',\n",
       " 'how',\n",
       " 'true',\n",
       " 'it',\n",
       " 'was',\n",
       " 'to',\n",
       " 'the',\n",
       " 'play',\n",
       " '.',\n",
       " '\\n \\n ',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxunk',\n",
       " 'is',\n",
       " 'around',\n",
       " 'here',\n",
       " 'xxunk',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'if',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'xxunk',\n",
       " 'in',\n",
       " 'finding',\n",
       " 'it',\n",
       " ',',\n",
       " 'fire',\n",
       " 'me',\n",
       " 'a',\n",
       " 'xxunk',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'if',\n",
       " 'i',\n",
       " 'can',\n",
       " 'get',\n",
       " 'you',\n",
       " 'the',\n",
       " 'xxunk',\n",
       " '.',\n",
       " 'xxunk']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise\n",
    "\n",
    "[movie_reviews.vocab.itos[a] for a in review.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Confirm that review has 81 distinct tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise\n",
    "\n",
    "len(set(review.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'street',\n",
       " 'impossible',\n",
       " 'clever',\n",
       " 'development',\n",
       " 'concept',\n",
       " 'william',\n",
       " 'worked',\n",
       " 'adventure',\n",
       " 'church',\n",
       " 'unlike',\n",
       " 'hold',\n",
       " 'lots',\n",
       " 'premise',\n",
       " 'shooting',\n",
       " 'washington',\n",
       " 'sick',\n",
       " 'effect',\n",
       " 'waiting',\n",
       " 'singing']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.vocab.itos[1000:1020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stoi` (string-to-int) is larger than `itos` (int-to-string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13150"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.vocab.stoi) - len(movie_reviews.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because many words are mapping to unknown.  We can confirm here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = []\n",
    "for word, num in movie_reviews.vocab.stoi.items():\n",
    "    if num==0:\n",
    "        unk.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13151"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'bleeping',\n",
       " 'pert',\n",
       " 'ticky',\n",
       " 'schtick',\n",
       " 'whoosh',\n",
       " 'banzai',\n",
       " 'chill',\n",
       " 'wooofff',\n",
       " 'cheery',\n",
       " 'superstars',\n",
       " 'fashionable',\n",
       " 'cruelly',\n",
       " 'separating',\n",
       " 'mistreat',\n",
       " 'tensions',\n",
       " 'religions',\n",
       " 'baseness',\n",
       " 'nobility',\n",
       " 'puro',\n",
       " 'disowned',\n",
       " 'option',\n",
       " 'faults',\n",
       " 'dignified',\n",
       " 'realisation',\n",
       " 'reconciliation',\n",
       " 'mrs',\n",
       " 'iyer',\n",
       " 'heartbreaking',\n",
       " 'histories',\n",
       " 'frankness',\n",
       " 'starters',\n",
       " 'montage',\n",
       " 'swearing',\n",
       " 'halestorm',\n",
       " 'korea',\n",
       " 'concentrate',\n",
       " 'pic',\n",
       " 'elude',\n",
       " 'characteristics',\n",
       " 'blathered',\n",
       " 'brassed',\n",
       " 'declaration',\n",
       " 'peck',\n",
       " 'garnered',\n",
       " 'fearless',\n",
       " 'tempered',\n",
       " 'humane',\n",
       " 'tails',\n",
       " 'slighted',\n",
       " 'slater',\n",
       " 'barrage',\n",
       " 'underway',\n",
       " 'operating',\n",
       " 'tag',\n",
       " 'dorff',\n",
       " 'reid',\n",
       " 'continually',\n",
       " 'revel',\n",
       " 'nra',\n",
       " 'benton',\n",
       " 'slate',\n",
       " 'penal',\n",
       " 'vengeful',\n",
       " 'seed',\n",
       " 'backbone',\n",
       " 'dismal',\n",
       " 'fortunate',\n",
       " 'ds',\n",
       " 'tmob',\n",
       " 'autographed',\n",
       " 'intercepted',\n",
       " 'lectured',\n",
       " 'reprints',\n",
       " 'comicon',\n",
       " 'attendees',\n",
       " 'blackhawk',\n",
       " 'insisted',\n",
       " 'jumped',\n",
       " 'apologized',\n",
       " 'wishing',\n",
       " 'seller',\n",
       " 'abomination',\n",
       " 'crib',\n",
       " 'seriousness',\n",
       " 'reclaim',\n",
       " 'sidenotes',\n",
       " 'archenemy',\n",
       " 'simultaneous',\n",
       " 'sheet',\n",
       " 'ely',\n",
       " 'leaping',\n",
       " 'brick',\n",
       " 'blasting',\n",
       " 'pistol',\n",
       " 'duster',\n",
       " 'postscript',\n",
       " 'accompanied',\n",
       " '1975',\n",
       " 'smack']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **log-count ratio** $r$ for each word $f$:\n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
    "\n",
    "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.y.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc\n",
    "y = movie_reviews.train.y\n",
    "val_y = movie_reviews.valid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = y.c2i['positive']\n",
    "negative = y.c2i['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010, 6010)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1), len(p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxunk'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7153,    0,  417,    0, ...,    0,    3,    3,    3], dtype=int64)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6468,    0,  383,    0, ...,    3,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(x[y.items==positive].sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6468,    0,  383,    0, ...,    3,    0,    0,    0], dtype=int64)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in our vocabulary, we are summing up how many positive reviews it is in, and how many negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6468,     0,   383,     0,     0, 10267,   674,    57,     0,  5260], dtype=int64)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = movie_reviews.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our ratios for even more data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use p0 and p1 to do some more data exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: compare how often \"loved\" appears in positive reviews vs. negative reviews.  How about \"hate\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 29)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: How often does the word \"loved\" appear in neg vs. pos reviews?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: How often does the word \"hated\" appear in neg vs. pos reviews?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### positive reviews with the word \"hated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was curious to look at an example of a postive review with the word \"hated\" in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1977"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.stoi['hated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15,  49, 304, 351, 393, 612, 695, 773], dtype=int32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.argwhere((x[:,1977] > 0))[:,0]; a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   3,  10,  11, ..., 787, 789, 790, 797])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.argwhere(y.items==positive)[:,0]; b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{393, 612, 695}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a).intersection(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj xxunk , yeah this episode is extremely underrated . \\n \\n  xxmaj even though there is a xxup lot of bad writing and acting at parts . i think the good over wins the bad . \\n \\n  i love the xxunk parts and the big ' twist ' at the end . i absolutely love that scene when xxmaj michelle xxunk xxmaj tony . xxmaj it 's actually one of my favorite scenes of xxmaj season 1 . \\n \\n  xxmaj for some reason , people have always hated the xxmaj xxunk episodes , yet i have always liked them . xxmaj they 're not the best , in terms of writing . but the theme really does interest me , \\n \\n  i 'm gon na give it a xxup three star , but if the writing were a little more consistent i 'd give it xxup four .\""
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = movie_reviews.train.x[695]\n",
    "review.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### negative reviews with the word \"loved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at an example of a negative review that contains the word \"loved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.stoi['loved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  19,  24,  51,  61,  70,  81, 110, 123, 155, 175, 193, 221, 265, 274, 279, 284, 290, 295, 304, 360, 384,\n",
       "       421, 465, 516, 520, 548, 569, 588, 604, 620, 631, 661, 672, 679, 702, 709, 759, 764, 792], dtype=int32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.argwhere((x[:,534] > 0))[:,0]; a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   5, ..., 795, 796, 798, 799])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.argwhere(y.items==negative)[:,0]; b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 24,\n",
       " 51,\n",
       " 70,\n",
       " 81,\n",
       " 123,\n",
       " 155,\n",
       " 193,\n",
       " 221,\n",
       " 274,\n",
       " 279,\n",
       " 284,\n",
       " 290,\n",
       " 295,\n",
       " 304,\n",
       " 421,\n",
       " 516,\n",
       " 548,\n",
       " 604,\n",
       " 620,\n",
       " 631,\n",
       " 672,\n",
       " 679,\n",
       " 709,\n",
       " 759,\n",
       " 764,\n",
       " 792}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a).intersection(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj this is not really a zombie film , if we \\'re xxunk zombies as the dead walking around . xxmaj here the protagonist , xxmaj xxunk xxmaj louque ( played by an xxunk young xxmaj dean xxmaj xxunk ) , xxunk control of a method to create zombies , though in fact , his \\' method \\' is to mentally project his thoughts and control other living people \\'s minds turning them into xxunk slaves . xxmaj this is an interesting concept for a movie , and was done much more effectively by xxmaj xxunk xxmaj lang in his series of \\' xxmaj dr. xxmaj mabuse \\' films , including \\' xxmaj dr. xxmaj mabuse the xxmaj xxunk \\' ( xxunk ) and \\' xxmaj the xxmaj testament of xxmaj dr. xxmaj mabuse \\' ( 1933 ) . xxmaj here it is unfortunately xxunk to his quest to regain the love of his former fiancée , xxmaj claire xxmaj duvall ( played by the xxmaj anne xxmaj xxunk look alike with a bad xxunk , xxmaj dorothy xxmaj stone ) which is really the major theme . \\n \\n  xxmaj the movie has an intriguing beginning , as xxmaj louque is sent on a military xxunk expedition to xxmaj xxunk to end the cult of zombies that came from there . xxmaj at some type of compound ( where we get great 30s sets and clothes ) he xxunk his xxunk to xxmaj claire , and then barely five minutes later , she gives him back his ring xxunk her love for his pal , xxmaj xxunk xxmaj greyson ( xxmaj robert xxmaj xxunk ) . xxmaj it \\'s unintentionally funny the way they talk to each other without making eye contact . xxmaj this would have been a great movie for \\' xxmaj mystery xxmaj science xxmaj theater xxunk \\' , if they had n\\'t already xxunk it . \\n \\n  xxmaj it \\'s never shown how xxmaj louque actually learns the \\' xxunk \\' secret , but he then uses it to kill his enemies , create a giant army of xxunk carrying soldiers and body guards . xxmaj we wo n\\'t see such sheer force of will until xxmaj john xxmaj xxunk in \\' xxmaj the xxmaj brain xxmaj from xxmaj planet xxmaj xxunk \\' ( xxunk ) . \\n \\n  xxmaj finally xxmaj claire xxunk to marry him if he will let xxmaj greyson live and return to xxmaj america . xxmaj louque agrees , but actually turns him into one of his xxunk slaves . xxmaj on their wedding night he realizes that xxmaj claire will only begin to love him if he gives up his \\' powers . \\' xxmaj to gain her love , he does so , causing the \\' revolt \\' of the title , in which all his slaves xxunk and attack his compound and kill him . xxmaj greyson xxunk xxmaj claire , and we seem to be at the end of a parable : \" xxmaj whom the xxunk would destroy , they first make mad . \" \\n \\n  xxmaj so really then , it \\'s not that bad of a film , despite the low imdb rating it currently has . xxmaj on repeated viewings ( ? ) one can see the xxunk in the well formed script ! xxmaj dean xxmaj xxunk had yet to develop into a good actor , and is almost unrecognizable in his xxunk -- is that really his own hair ? xxmaj we remember him more for his xxunk , old man roles in \\' xxmaj white xxmaj christmas \\' ( xxunk ) , \\' x xxmaj the xxmaj unknown \\' ( 1956 ) and \\' xxmaj king xxmaj xxunk \\' ( 1958 ) . xxmaj the story xxunk a lot of its basic themes from the xxmaj xxunk brothers better , earlier film \\' xxmaj white xxmaj zombie \\' ( xxunk ) in which xxunk xxmaj robert xxmaj xxunk ( as xxmaj charles xxmaj xxunk ) uses \\' xxunk \\' to win the love of xxmaj xxunk xxmaj xxunk ( as xxmaj xxunk xxmaj parker ) . \\n \\n  xxmaj if you want real zombie movies ( of which there are hundreds ! ) i \\'d start with \\' xxmaj white xxmaj zombie \\' ( xxunk ) , \\' xxmaj king of the xxmaj zombies \\' ( xxunk ) , \\' i xxmaj walked with a xxmaj zombie \\' ( xxunk ) , \\' xxmaj night of the xxmaj living xxmaj dead \\' ( xxunk ) , \\' xxmaj the xxmaj last xxmaj man on xxmaj earth \\' ( 1964 ) and its two xxunk . xxmaj in the modern era of classy films , there are \\' xxmaj horror xxmaj express \\' ( 1972 ) , \\' xxmaj the xxmaj xxunk and the xxmaj xxunk \\' ( xxunk ) , \\' 28 xxmaj days xxmaj later \\' ( 2002 ) and its sequel , as well as many , many , others too numerous to mention . \\n \\n  xxmaj this one is not really a zombie film . xxmaj judging this movie on its own terms , it \\'s more of a semi - xxmaj gothic romance . xxmaj as such it ranks a little below some of xxmaj universal \\'s bottom billed b horror movies of the late 30s and early xxunk . xxmaj so i \\'ll give it a 5 .'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = movie_reviews.train.x[792]\n",
    "review.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.015811,  0.084839,  0.      ,  0.084839, ...,  1.471133, -1.301455, -1.301455, -1.301455])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.log(pr1/pr0); r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab most likely associated with positive/negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest = np.argpartition(r, -10)[-10:]\n",
    "smallest = np.argpartition(r, 10)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most positive words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paxton',\n",
       " 'gilliam',\n",
       " 'davies',\n",
       " 'jabba',\n",
       " 'jimmy',\n",
       " 'felix',\n",
       " 'biko',\n",
       " 'fanfan',\n",
       " 'astaire',\n",
       " 'noir']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.itos[k] for k in biggest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(trn_term_doc[:,v.stoi['biko']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos \" xxmaj the xxmaj true xxmaj story xxmaj of xxmaj the xxmaj friendship xxmaj that xxmaj shook xxmaj south xxmaj africa xxmaj and xxmaj xxunk xxmaj the xxmaj world . \" \n",
       " \n",
       "  xxmaj richard xxmaj attenborough , who directed \" a xxmaj bridge xxmaj too xxmaj far \" and \" xxmaj gandhi \" , wanted to bring the story of xxmaj steve xxmaj biko to life , and the journey and trouble that xxunk xxmaj donald xxmaj woods went through in order to get his story told . xxmaj the films uses xxmaj wood 's two books for it 's information and basis - \" xxmaj biko \" and \" xxmaj asking for xxmaj trouble \" . \n",
       " \n",
       "  xxmaj the film takes place in the late 1970 's , in xxmaj south xxmaj africa . xxmaj south xxmaj africa is in the grip of the terrible apartheid , which keeps the blacks separated from the whites and xxunk the whites as the superior race . xxmaj the blacks are forced to live in xxunk on the xxunk of the cities and xxunk , and they come under frequent xxunk by the police and the army . xxmaj we are shown a dawn xxunk on a xxunk , as xxunk and armed police force their way through the camp beating and even killing the inhabitants . xxmaj then we are introduced to xxmaj donald xxmaj woods ( xxmaj kevin xxmaj kline ) , who is the editor of a popular newspaper . xxmaj after xxunk a negative story about black xxunk xxmaj steve xxmaj biko ( xxmaj denzel xxmaj washington ) , xxmaj woods goes to meet with him . xxmaj the two are xxunk of each other at first , but they soon become good friends and xxmaj biko shows the horrors of the apartheid system from a black persons point of view to xxmaj woods . xxmaj this xxunk xxmaj woods to speak out against what 's happening around him , and makes him desperate to bring xxmaj steve xxmaj biko 's story out of the xxunk of the white man 's xxmaj south xxmaj africa and to the world . xxmaj soon , xxmaj steve xxmaj biko is arrested and is killed in prison . xxmaj now xxmaj woods and his family are daring to escape from xxmaj south xxmaj africa to xxmaj england , where xxmaj woods can xxunk his book about xxmaj steve xxmaj biko and the apartheid . \n",
       " \n",
       "  xxmaj when i first heard of \" xxmaj cry xxmaj freedom \" , i was under the impression that it was a movie completely dedicated to the life of xxmaj steve xxmaj biko . i had never actually heard of xxmaj steve xxmaj biko before i seen this film , as the events in this film were really before my time . xxmaj but it 's more about the story of xxmaj donald xxmaj woods and his journey across the border into xxmaj xxunk as he tried to xxunk the xxmaj south xxmaj african xxunk . xxmaj woods was put on a five year type house xxunk after xxmaj steve xxmaj biko was killed . xxmaj so in order to xxunk his xxunk on xxmaj steve xxmaj biko , he had to escape . xxmaj because the xxunk would be considered xxunk in xxmaj south xxmaj africa and that could have resulted in xxmaj woods meeting a fate similar to that of xxmaj biko 's . xxmaj the real xxmaj donald xxmaj woods and his wife acted as xxunk to this film . \n",
       " \n",
       "  xxmaj denzel xxmaj washington is only in the film for the first hour , and i was disappointed with that as i was expecting to see him for the entire movie . xxmaj but he was amazing as xxmaj steve xxmaj biko , and captured his personality from what i 've read really well and his accent sounded perfect . xxmaj his performance earned him an xxmaj oscar nomination for xxmaj best xxmaj supporting xxmaj actor . xxmaj kevin xxmaj kline delivers a excellent and thought - xxunk performance as xxmaj donald xxmaj woods , and xxmaj penelope xxmaj xxunk is excellent as his wife xxmaj xxunk . \n",
       " \n",
       "  xxmaj filming took place in xxmaj xxunk , as needless to say problems xxunk when they tried to film it in xxmaj south xxmaj africa . xxmaj while in xxmaj south xxmaj africa , the xxmaj south xxmaj african xxunk followed the film crew everywhere , so they got the bad xxunk and they pulled out and went to xxunk xxmaj xxunk instead . xxmaj despite everything , and the fact that the apartheid did n't end ' xxunk seven years later , \" xxmaj cry xxmaj freedom \" was n't xxunk in xxmaj south xxmaj africa . xxmaj but xxunk showing the movie received bomb threats . \n",
       " \n",
       "  xxmaj richard xxmaj attenborough brings the horrors of the apartheid to the screen with extreme force and determination . xxmaj he does n't hold back at the end of the movie when showing what was supposed to be a xxunk xxunk by students in a xxunk , turns into a massacre when police open fire on them . xxmaj the film ends with the names of all the anti - apartheid xxunk who died in prison , and the explanations for their deaths . xxmaj many had \" xxmaj no xxmaj explanation \" . xxmaj quite a few were \" xxmaj xxunk \" , which is hard to believe , and many more either fell from the top of the xxunk or were \" xxmaj suicide from xxmaj hanging \" . xxmaj no one will ever know what really happened to them , but i think it 's fair to say that none of these men died at their own hands , but at the hands of others ; or to be more xxunk , at the hands of the police . \n",
       " \n",
       "  \" xxmaj cry xxmaj freedom \" is a must - see movie for it 's portrayal and story of xxmaj steve xxmaj biko . xxmaj it 's also a xxunk and xxunk portrayal of a beautiful land divided and in the xxunk grips of racial xxunk and violence ."
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.train.x[515]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog',\n",
       " 'disappointment',\n",
       " 'naschy',\n",
       " 'vargas',\n",
       " 'worst',\n",
       " 'crap',\n",
       " 'crater',\n",
       " 'porn',\n",
       " 'soderbergh',\n",
       " 'fuqua']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.itos[k] for k in smallest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(trn_term_doc[:,v.stoi['soderbergh']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \n",
       " \n",
       "  xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after xxunk years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \n",
       " \n",
       "  xxmaj none of this excuses him this present , almost diabolical failure . xxmaj as xxmaj david xxmaj xxunk xxunk , \" two parts of xxmaj che do n't ( even ) make a whole \" . \n",
       " \n",
       "  xxmaj epic xxunk in name only , xxmaj che(2008 ) barely qualifies as a feature film ! xxmaj it certainly has no legs , xxunk as except for its xxunk ultimate resolution forced upon it by history , xxmaj soderbergh 's xxunk - long xxunk just goes nowhere . \n",
       " \n",
       "  xxmaj even xxmaj margaret xxmaj xxunk , the more xxunk of xxmaj australia 's xxmaj at xxmaj the xxmaj movies duo , noted about xxmaj soderbergh 's xxunk waste of ( xxup xxunk digital xxunk ) : \" you 're in the woods ... xxunk in the woods ... xxunk in the woods ... \" . i too am surprised xxmaj soderbergh did n't give us another xxunk of xxup that somewhere between his xxunk two xxmaj parts , because he still left out massive xxunk of xxmaj che 's \" xxunk \" life ! \n",
       " \n",
       "  xxmaj for a xxunk of an important but infamous historical figure , xxmaj soderbergh xxunk xxunk , if not deliberately insults , his audiences by \n",
       " \n",
       "  1 . never providing most of xxmaj che 's story ; \n",
       " \n",
       "  2 . xxunk xxunk film xxunk with mere xxunk xxunk ; \n",
       " \n",
       "  3 . xxunk both true xxunk and a narrative of events ; \n",
       " \n",
       "  4 . barely developing an idea , or a character ; \n",
       " \n",
       "  5 . remaining xxunk episodic ; \n",
       " \n",
       "  6 . xxunk proper context for scenes --- whatever we do get is xxunk in xxunk xxunk ; \n",
       " \n",
       "  7 . xxunk xxunk all audiences ( even xxmaj spanish - xxunk will be confused by the xxunk xxunk in xxmaj english ) ; and \n",
       " \n",
       "  8 . xxunk xxunk his main subject into one dimension . xxmaj why , at xxup this late stage ? xxmaj the t - shirt franchise has been a success ! \n",
       " \n",
       "  xxmaj our sense of xxunk is surely due to xxmaj peter xxmaj xxunk and xxmaj benjamin xxunk xxmaj xxunk xxunk their screenplay solely on xxmaj xxunk 's memoirs . xxmaj so , like a poor student who has read only xxup one of his xxunk xxunk for his xxunk , xxmaj soderbergh 's product is xxunk limited in perspective . \n",
       " \n",
       "  xxmaj the audience is held captive within the same xxunk knowledge , scenery and circumstances of the \" revolutionaries \" , but that does n't xxunk our sympathy . xxmaj instead , it xxunk on us that \" xxmaj ah , xxmaj soderbergh 's trying to xxunk his audiences the same as the xxmaj latino peasants were at the time \" . xxmaj but these are the xxup same illiterate xxmaj latino peasants who xxunk out the good doctor to his enemies . xxmaj why does xxmaj soderbergh feel the need to xxunk us with them , and keep us equally mentally captive ? xxmaj such audience xxunk must have a purpose . \n",
       " \n",
       "  xxmaj part2 is more xxunk than xxmaj part1 , but it 's literally mind - numbing with its repetitive bush - bashing , misery of xxunk , and lack of variety or character xxunk . deltoro 's xxmaj che has no opportunity to grow as a person while he struggles to xxunk his own ill - xxunk troops . xxmaj the only xxunk is the humour as xxmaj che deals with his sometimes deeply ignorant \" revolutionaries \" , some of whom xxunk lack self - control around local peasants or food . xxmaj we certainly get no insight into what caused the conditions , nor any xxunk xxunk of their xxunk xxunk , such as it was . \n",
       " \n",
       "  xxmaj part2 's xxunk xxunk remains xxunk episodic : again , nothing is telegraphed or xxunk . xxmaj thus even the scenes with xxmaj xxunk xxmaj xxunk ( xxmaj xxunk xxmaj xxunk ) are unexpected and disconcerting . xxmaj any xxunk events are portrayed xxunk and xxmaj latino - xxunk , with xxmaj part1 's interviews xxunk by time - xxunk xxunk between the corrupt xxmaj xxunk president ( xxmaj xxunk de xxmaj xxunk ) and xxup us xxmaj government xxunk promising xxup cia xxunk ( ! ) . \n",
       " \n",
       "  xxmaj the rest of xxmaj part2 's \" woods \" and day - for - night blue xxunk just xxunk the audience until they 're xxunk the xxunk . \n",
       " \n",
       "  xxmaj perhaps deltoro felt too xxunk the frustration of many non - xxmaj american xxmaj latinos about never getting a truthful , xxunk history of xxmaj che 's xxunk within their own countries . xxmaj when foreign xxunk still wo n't deliver a free press to their people -- for whatever reason -- then one can see how a popular xxmaj american indie producer might set out to xxunk the not - so - well - read ( \" i may not be able to read or write , but i 'm xxup not xxunk . xxmaj the xxmaj inspector xxmaj xxunk ) ) out to their own local xxunk . xxmaj the film 's obvious xxunk and gross over - xxunk hint very strongly that it 's aiming only at the xxunk of the less - informed xxup who xxup still xxup speak xxup little xxmaj english . xxmaj if they did , they 'd have read xxunk on the subject already , and xxunk the relevant social issues amongst themselves -- learning the lessons of history as they should . \n",
       " \n",
       "  xxmaj such insights are precisely what societies still need -- and not just the remaining illiterate xxmaj latinos of xxmaj central and xxmaj south xxmaj america -- yet it 's what xxmaj che(2008 ) xxunk fails to deliver . xxmaj soderbergh xxunk his lead because he 's weak on narrative . i am xxunk why xxmaj xxunk deltoro deliberately chose xxmaj soderbergh for this project if he knew this . xxmaj it 's been xxunk , xxunk about xxmaj xxunk was xxunk wanted : it 's what i went to see this film for , but the director xxunk robs us of that . \n",
       " \n",
       "  xxmaj david xxmaj xxunk , writing in xxmaj the xxmaj australian ( xxunk ) observed that while xxmaj part1 was \" uneven \" , xxmaj part2 actually \" goes rapidly downhill \" from there , \" xxunk xxmaj che 's final xxunk in xxmaj xxunk in xxunk detail \" , which \" ... feels almost unbearably slow and turgid \" . \n",
       " \n",
       "  xxmaj che : xxmaj the xxmaj xxunk aka xxmaj part2 is certainly no xxunk for xxmaj xxunk , painting it a picture of misery and xxunk . xxmaj the entire second half is only xxunk by the aforementioned humour , and the dramatic -- yet tragic -- capture and execution of the film 's subject . \n",
       " \n",
       "  xxmaj the rest of this xxunk cinema xxunk is just confusing , irritating misery -- xxunk , for a xxmaj soderbergh film , to be avoided at all costs . xxmaj it is bound to break the hearts of all who know even just a xxunk about the xxunk / 10 )"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.train.x[434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[:,v.stoi['soderbergh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog',\n",
       " 'disappointment',\n",
       " 'naschy',\n",
       " 'vargas',\n",
       " 'worst',\n",
       " 'crap',\n",
       " 'crater',\n",
       " 'porn',\n",
       " 'soderbergh',\n",
       " 'fuqua']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.itos[k] for k in smallest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47875, 0.52125)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y.items==positive).mean(), (y.items==negative).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (val_term_doc @ r + b) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == val_y.items).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching to full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our approach working on a smaller sample of the data, we can try using it on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/racheltho/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/models'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/imdb_textlist_class'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/lm_databunch')]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/racheltho/.fastai/data/imdb/train/pos'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/train/unsupBow.feat'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " PosixPath('/home/racheltho/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full = (TextList.from_folder(path)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos']))\n",
    "             #label them all with their folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_full.train), len(reviews_full.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the vocab in a variable `v` since we will be using it frequently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = reviews_full.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'people',\n",
       " 'will',\n",
       " 'other',\n",
       " 'also',\n",
       " 'into',\n",
       " 'first',\n",
       " 'because',\n",
       " 'great',\n",
       " 'how']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.itos[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.57 s, sys: 116 ms, total: 4.69 s\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_term_doc = get_term_doc_matrix(reviews_full.valid.x, len(reviews_full.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.1 s, sys: 236 ms, total: 5.34 s\n",
      "Wall time: 5.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_term_doc = get_term_doc_matrix(reviews_full.train.x, len(reviews_full.vocab.itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was slow.  Let's save our matrices for faster loading next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"trn_term_doc.npz\", trn_term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"val_term_doc.npz\", val_term_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When storing data like this, always make sure it's included in your .gitignore file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we'll just be able to load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = scipy.sparse.load_npz(\"trn_term_doc.npz\")\n",
    "val_term_doc = scipy.sparse.load_npz(\"val_term_doc.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc\n",
    "y=reviews_full.train.y\n",
    "\n",
    "val_y = reviews_full.valid.y.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x38457 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3716501 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = y.c2i['pos']\n",
    "negative = y.c2i['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))\n",
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 28399,      0,  12500,      0,      0, 342576,  20471,   1338,      7, 173118, 137954, 143763,  89571,  83406,\n",
       "        76828,  66715,  58511,  47892,  50178,  40448], dtype=int64)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration: negative to positive ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was curious about the ratio of times a given word appears in negative reviews to times it occurs in positive reviews.  Bigger ratios (> 1) mean the word is indicative of a negative review, and smaller ratios (< 1) mean it is indicative of a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_pos_given_word(word):\n",
    "    print(p0[v.stoi[word]]/p1[v.stoi[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.051546391752577\n"
     ]
    }
   ],
   "source": [
    "neg_pos_given_word('hated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6424702058504875\n"
     ]
    }
   ],
   "source": [
    "neg_pos_given_word('liked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3139963167587477\n"
     ]
    }
   ],
   "source": [
    "neg_pos_given_word('loved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48538961038961037\n"
     ]
    }
   ],
   "source": [
    "neg_pos_given_word('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.837301587301587\n"
     ]
    }
   ],
   "source": [
    "neg_pos_given_word('worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1/pr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7133498878774648"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[v.stoi['hated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1563661500586044"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[v.stoi['loved']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2826243504315076"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[v.stoi['worst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225576052173609"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[v.stoi['best']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = y.c2i['neg']\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have equal numbers of positive and negative reviews in this data set, b is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean()); b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (val_term_doc @ r + b) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is 80% for the full data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80864"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it only matters whether a word is in the review or not (not the frequency of the word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc.sign()\n",
    "y=reviews_full.train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 0, ..., 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, ..., 0, 0, 0, 1],\n",
       "        [1, 0, 1, 0, ..., 1, 0, 0, 1],\n",
       "        [1, 0, 1, 0, ..., 0, 0, 0, 1],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, ..., 0, 0, 0, 1],\n",
       "        [1, 0, 1, 0, ..., 1, 0, 0, 1],\n",
       "        [1, 0, 1, 0, ..., 1, 0, 0, 1],\n",
       "        [0, 0, 1, 0, ..., 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.todense()[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = y.c2i['neg']\n",
    "positive = y.c2i['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1/pr0)\n",
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean())\n",
    "\n",
    "preds = (val_term_doc.sign() @ r + b) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82908"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can fit logistic regression where the features are the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8828"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y.items.astype(int))\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the binarized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88528"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(trn_term_doc.sign(), y.items.astype(int))\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram with NB features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-gram is a contiguous sequence of n items (where the items can be characters, syllables, or words).  A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram.\n",
    "\n",
    "Here, we are referring to sequences of words. So examples of bigrams include \"the dog\", \"said that\", and \"can't you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                .split_from_df(col=2)\n",
    "                .label_from_df(cols=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = movie_reviews.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n=1\n",
    "max_n=3\n",
    "\n",
    "j_indices = []\n",
    "indptr = []\n",
    "values = []\n",
    "indptr.append(0)\n",
    "num_tokens = vocab_len\n",
    "\n",
    "itongram = dict()\n",
    "ngramtoi = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate through the sequences of words to create our n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(movie_reviews.train.x):\n",
    "    feature_counter = Counter(doc.data)\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    this_doc_ngrams = list()\n",
    "\n",
    "    m = 0\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for k in range(vocab_len - n + 1):\n",
    "            ngram = doc.data[k: k + n]\n",
    "            if str(ngram) not in ngramtoi:\n",
    "                if len(ngram)==1:\n",
    "                    num = ngram[0]\n",
    "                    ngramtoi[str(ngram)] = num\n",
    "                    itongram[num] = ngram\n",
    "                else:\n",
    "                    ngramtoi[str(ngram)] = num_tokens\n",
    "                    itongram[num_tokens] = ngram\n",
    "                    num_tokens += 1\n",
    "            this_doc_ngrams.append(ngramtoi[str(ngram)])\n",
    "            m += 1\n",
    "\n",
    "    ngram_counter = Counter(this_doc_ngrams)\n",
    "    j_indices.extend(ngram_counter.keys())\n",
    "    values.extend(ngram_counter.values())\n",
    "    indptr.append(len(j_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dictionaries to convert between indices and strings (in this case, our n-grams) is a common & useful approach!  Here, we have `itongram` (index to n-gram) and `ngramtoi` (n-gram to index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram_doc_matrix = scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
    "                                   shape=(len(indptr) - 1, len(ngramtoi)),\n",
    "                                   dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260402 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 678912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260428, 260428)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngramtoi), len(itongram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([637,   0,  59])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itongram[20005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20005"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramtoi[str(np.array([637,   0,  59]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 189, 1301])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itongram[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nothing', 'changes')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[189], v[1301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63, 48])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itongram[100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('has', 'an')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[63], v[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([94, 36, 84])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itongram[6116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('does', \"n't\", 'even')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[94], v[36], v[84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190,  62, 935])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itongram[6119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('look', 'her', 'usual')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[190], v[62], v[935]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 805,   11, 1202])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itongram[80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('business', 'and', 'political')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[805], v[11], v[1202]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create valid matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_indices = []\n",
    "indptr = []\n",
    "values = []\n",
    "indptr.append(0)\n",
    "\n",
    "for i, doc in enumerate(movie_reviews.valid.x):\n",
    "    feature_counter = Counter(doc.data)\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    this_doc_ngrams = list()\n",
    "\n",
    "    m = 0\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for k in range(vocab_len - n + 1):\n",
    "            ngram = doc.data[k: k + n]\n",
    "            if str(ngram) in ngramtoi:\n",
    "                this_doc_ngrams.append(ngramtoi[str(ngram)])\n",
    "            m += 1\n",
    "\n",
    "    ngram_counter = Counter(this_doc_ngrams)\n",
    "    j_indices.extend(ngram_counter.keys())\n",
    "    values.extend(ngram_counter.values())\n",
    "    indptr.append(len(j_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ngram_doc_matrix = scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
    "                                   shape=(len(indptr) - 1, len(ngramtoi)),\n",
    "                                   dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200x260402 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 121597 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ngram_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260402 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 678912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"train_ngram_matrix.npz\", train_ngram_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"valid_ngram_matrix.npz\", valid_ngram_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('itongram.pickle', 'wb') as handle:\n",
    "    pickle.dump(itongram, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('ngramtoi.pickle', 'wb') as handle:\n",
    "    pickle.dump(itongram, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_ngram_doc_matrix = scipy.sparse.load_npz(\"train_ngram_matrix.npz\")\n",
    "valid_ngram_doc_matrix = scipy.sparse.load_npz(\"valid_ngram_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('itongram.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "    \n",
    "with open('ngramtoi.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_ngram_doc_matrix\n",
    "y=movie_reviews.train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = y.c2i['positive']\n",
    "negative = y.c2i['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260402 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 678912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=260428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (y.items == positive)[:k]\n",
    "neg = (y.items == negative)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [o == positive for o in movie_reviews.valid.y.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
    "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1/pr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08505123261815539"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47875, 0.52125)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y.items==positive).mean(), (y.items==negative).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_preds = valid_ngram_doc_matrix @ r.T + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 110.972709,   39.540612,    1.138566,   14.709707, ...,   81.056242,   -5.947489, -152.217182,  120.186321])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pre_preds.T>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False,  True,  True, False,  True, False])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [o == positive for o in movie_reviews.valid.y.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x_ngram_sgn = train_ngram_doc_matrix.sign()\n",
    "val_x_ngram_sgn = valid_ngram_doc_matrix.sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = trn_x_ngram_sgn[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
    "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1/pr0)\n",
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean())\n",
    "\n",
    "pre_preds = val_x_ngram_sgn @ r.T + b\n",
    "preds = pre_preds.T>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds==valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the features are the trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use CountVectorizer to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(ngram_range=(1,3), preprocessor=noop, tokenizer=noop, max_features=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = movie_reviews.train.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [[docs.vocab.itos[o] for o in doc.data] for doc in movie_reviews.train.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = [[docs.vocab.itos[o] for o in doc.data] for doc in movie_reviews.valid.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 8 ms, total: 1.22 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ngram_doc = veczr.fit_transform(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260401 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 565699 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xxbos': 235245,\n",
       " 'xxmaj': 235619,\n",
       " 'un': 217541,\n",
       " '-': 14669,\n",
       " 'xxunk': 247983,\n",
       " 'believable': 50437,\n",
       " '!': 593,\n",
       " 'meg': 134464,\n",
       " 'ryan': 171981,\n",
       " 'does': 72638,\n",
       " \"n't\": 141217,\n",
       " 'even': 78300,\n",
       " 'look': 129031,\n",
       " 'her': 101696,\n",
       " 'usual': 219431,\n",
       " 'lovable': 129884,\n",
       " 'self': 175890,\n",
       " 'in': 110078,\n",
       " 'this': 206648,\n",
       " ',': 8808,\n",
       " 'which': 228228,\n",
       " 'normally': 145208,\n",
       " 'makes': 131572,\n",
       " 'me': 133672,\n",
       " 'forgive': 88683,\n",
       " 'shallow': 177184,\n",
       " 'acting': 27693,\n",
       " '.': 16845,\n",
       " 'hard': 97877,\n",
       " 'to': 210394,\n",
       " 'believe': 50474,\n",
       " 'she': 177332,\n",
       " 'was': 222339,\n",
       " 'the': 193825,\n",
       " 'producer': 164509,\n",
       " 'on': 152345,\n",
       " 'dog': 72943,\n",
       " 'plus': 162153,\n",
       " 'kevin': 122626,\n",
       " 'kline': 123483,\n",
       " ':': 20369,\n",
       " 'what': 226885,\n",
       " 'kind': 123253,\n",
       " 'of': 147538,\n",
       " 'suicide': 188380,\n",
       " 'trip': 215838,\n",
       " 'has': 98145,\n",
       " 'his': 103548,\n",
       " 'career': 58706,\n",
       " 'been': 49251,\n",
       " '?': 20997,\n",
       " '...': 18350,\n",
       " 'finally': 85389,\n",
       " 'directed': 71229,\n",
       " 'by': 56573,\n",
       " 'guy': 96432,\n",
       " 'who': 229053,\n",
       " 'did': 70485,\n",
       " 'big': 51612,\n",
       " 'must': 140433,\n",
       " 'be': 47429,\n",
       " 'a': 21285,\n",
       " 'replay': 169637,\n",
       " 'jonestown': 121196,\n",
       " 'hollywood': 105205,\n",
       " 'style': 187631,\n",
       " 'xxbos xxmaj': 235387,\n",
       " 'xxmaj un': 246414,\n",
       " 'un -': 217542,\n",
       " '- xxunk': 16431,\n",
       " 'xxunk -': 248730,\n",
       " '- believable': 14857,\n",
       " 'believable !': 50438,\n",
       " '! xxmaj': 756,\n",
       " 'xxmaj meg': 242214,\n",
       " 'meg xxmaj': 134469,\n",
       " 'xxmaj ryan': 244169,\n",
       " 'ryan does': 171990,\n",
       " \"does n't\": 72770,\n",
       " \"n't even\": 141546,\n",
       " 'even look': 78540,\n",
       " 'look her': 129116,\n",
       " 'her usual': 102342,\n",
       " 'usual xxunk': 219463,\n",
       " 'xxunk lovable': 252472,\n",
       " 'lovable self': 129893,\n",
       " 'self in': 175915,\n",
       " 'in this': 111802,\n",
       " 'this ,': 206667,\n",
       " ', which': 13819,\n",
       " 'which normally': 228500,\n",
       " 'normally makes': 145211,\n",
       " 'makes me': 131651,\n",
       " 'me forgive': 133807,\n",
       " 'forgive her': 88684,\n",
       " 'her shallow': 102248,\n",
       " 'shallow xxunk': 177199,\n",
       " 'xxunk acting': 249152,\n",
       " 'acting xxunk': 27935,\n",
       " 'xxunk .': 248830,\n",
       " '. xxmaj': 17378,\n",
       " 'xxmaj hard': 239935,\n",
       " 'hard to': 97933,\n",
       " 'to believe': 210927,\n",
       " 'believe she': 50543,\n",
       " 'she was': 177776,\n",
       " 'was the': 223713,\n",
       " 'the producer': 199567,\n",
       " 'producer on': 164521,\n",
       " 'on this': 153116,\n",
       " 'this dog': 206987,\n",
       " 'dog .': 72944,\n",
       " 'xxmaj plus': 243544,\n",
       " 'plus xxmaj': 162173,\n",
       " 'xxmaj kevin': 241367,\n",
       " 'kevin xxmaj': 122631,\n",
       " 'xxmaj kline': 241463,\n",
       " 'kline :': 123489,\n",
       " ': what': 20599,\n",
       " 'what kind': 227138,\n",
       " 'kind of': 123276,\n",
       " 'of suicide': 149900,\n",
       " 'suicide trip': 188402,\n",
       " 'trip has': 215849,\n",
       " 'has his': 98415,\n",
       " 'his career': 103726,\n",
       " 'career been': 58722,\n",
       " 'been on': 49462,\n",
       " 'on ?': 152394,\n",
       " '? xxmaj': 21121,\n",
       " 'xxmaj xxunk': 247340,\n",
       " 'xxunk ...': 248876,\n",
       " '... xxmaj': 18634,\n",
       " 'xxunk !': 247988,\n",
       " '! !': 608,\n",
       " 'xxmaj finally': 239085,\n",
       " 'finally this': 85451,\n",
       " 'this was': 208191,\n",
       " 'was directed': 222794,\n",
       " 'directed by': 71242,\n",
       " 'by the': 56992,\n",
       " 'the guy': 197105,\n",
       " 'guy who': 96523,\n",
       " 'who did': 229250,\n",
       " 'did xxmaj': 70760,\n",
       " 'xxmaj big': 236752,\n",
       " 'big xxmaj': 51762,\n",
       " 'xxunk ?': 249000,\n",
       " 'xxmaj must': 242544,\n",
       " 'must be': 140446,\n",
       " 'be a': 47474,\n",
       " 'a replay': 24656,\n",
       " 'replay of': 169640,\n",
       " 'of xxmaj': 150935,\n",
       " 'xxmaj jonestown': 241169,\n",
       " 'jonestown -': 121197,\n",
       " '- hollywood': 15348,\n",
       " 'hollywood style': 105278,\n",
       " 'style .': 187642,\n",
       " 'xxbos xxmaj un': 235573,\n",
       " 'xxmaj un -': 246415,\n",
       " 'un - xxunk': 217545,\n",
       " '- xxunk -': 16437,\n",
       " 'xxunk - believable': 248740,\n",
       " '- believable !': 14858,\n",
       " 'believable ! xxmaj': 50439,\n",
       " '! xxmaj meg': 813,\n",
       " 'xxmaj meg xxmaj': 242217,\n",
       " 'meg xxmaj ryan': 134471,\n",
       " 'xxmaj ryan does': 244174,\n",
       " \"ryan does n't\": 171991,\n",
       " \"does n't even\": 72785,\n",
       " \"n't even look\": 141562,\n",
       " 'even look her': 78542,\n",
       " 'look her usual': 129117,\n",
       " 'her usual xxunk': 102344,\n",
       " 'usual xxunk lovable': 219466,\n",
       " 'xxunk lovable self': 252473,\n",
       " 'lovable self in': 129894,\n",
       " 'self in this': 175916,\n",
       " 'in this ,': 111804,\n",
       " 'this , which': 206693,\n",
       " ', which normally': 13851,\n",
       " 'which normally makes': 228501,\n",
       " 'normally makes me': 145212,\n",
       " 'makes me forgive': 131655,\n",
       " 'me forgive her': 133808,\n",
       " 'forgive her shallow': 88685,\n",
       " 'her shallow xxunk': 102249,\n",
       " 'shallow xxunk acting': 177200,\n",
       " 'xxunk acting xxunk': 249154,\n",
       " 'acting xxunk .': 27936,\n",
       " 'xxunk . xxmaj': 248864,\n",
       " '. xxmaj hard': 17676,\n",
       " 'xxmaj hard to': 239936,\n",
       " 'hard to believe': 97940,\n",
       " 'to believe she': 210934,\n",
       " 'believe she was': 50544,\n",
       " 'she was the': 177809,\n",
       " 'was the producer': 223755,\n",
       " 'the producer on': 199569,\n",
       " 'producer on this': 164522,\n",
       " 'on this dog': 153121,\n",
       " 'this dog .': 206988,\n",
       " 'dog . xxmaj': 72946,\n",
       " '. xxmaj plus': 17920,\n",
       " 'xxmaj plus xxmaj': 243548,\n",
       " 'plus xxmaj kevin': 162176,\n",
       " 'xxmaj kevin xxmaj': 241370,\n",
       " 'kevin xxmaj kline': 122633,\n",
       " 'xxmaj kline :': 241466,\n",
       " 'kline : what': 123490,\n",
       " ': what kind': 20600,\n",
       " 'what kind of': 227139,\n",
       " 'kind of suicide': 123325,\n",
       " 'of suicide trip': 149902,\n",
       " 'suicide trip has': 188403,\n",
       " 'trip has his': 215850,\n",
       " 'has his career': 98417,\n",
       " 'his career been': 103730,\n",
       " 'career been on': 58723,\n",
       " 'been on ?': 49463,\n",
       " 'on ? xxmaj': 152395,\n",
       " '? xxmaj xxunk': 21220,\n",
       " 'xxmaj xxunk ...': 247355,\n",
       " 'xxunk ... xxmaj': 248895,\n",
       " '... xxmaj xxunk': 18669,\n",
       " 'xxmaj xxunk !': 247341,\n",
       " 'xxunk ! !': 247990,\n",
       " '! ! !': 610,\n",
       " '! ! xxmaj': 615,\n",
       " '! xxmaj finally': 793,\n",
       " 'xxmaj finally this': 239090,\n",
       " 'finally this was': 85452,\n",
       " 'this was directed': 208199,\n",
       " 'was directed by': 222795,\n",
       " 'directed by the': 71243,\n",
       " 'by the guy': 57034,\n",
       " 'the guy who': 197110,\n",
       " 'guy who did': 96527,\n",
       " 'who did xxmaj': 229253,\n",
       " 'did xxmaj big': 70762,\n",
       " 'xxmaj big xxmaj': 236757,\n",
       " 'big xxmaj xxunk': 51769,\n",
       " 'xxmaj xxunk ?': 247369,\n",
       " 'xxunk ? xxmaj': 249011,\n",
       " '? xxmaj must': 21172,\n",
       " 'xxmaj must be': 242545,\n",
       " 'must be a': 140448,\n",
       " 'be a replay': 47519,\n",
       " 'a replay of': 24657,\n",
       " 'replay of xxmaj': 169642,\n",
       " 'of xxmaj jonestown': 151078,\n",
       " 'xxmaj jonestown -': 241170,\n",
       " 'jonestown - hollywood': 121198,\n",
       " '- hollywood style': 15349,\n",
       " 'hollywood style .': 105279,\n",
       " 'style . xxmaj': 187643,\n",
       " '. xxmaj xxunk': 18197,\n",
       " 'is': 114388,\n",
       " 'extremely': 80780,\n",
       " 'well': 225746,\n",
       " 'made': 130666,\n",
       " 'film': 83839,\n",
       " 'script': 174300,\n",
       " 'and': 33959,\n",
       " 'camera': 57738,\n",
       " 'work': 233513,\n",
       " 'are': 41025,\n",
       " 'all': 30474,\n",
       " 'first': 85911,\n",
       " 'rate': 166635,\n",
       " 'music': 140267,\n",
       " 'good': 94167,\n",
       " 'too': 214437,\n",
       " 'though': 208639,\n",
       " 'it': 117573,\n",
       " 'mostly': 137870,\n",
       " 'early': 74996,\n",
       " 'when': 227487,\n",
       " 'things': 206104,\n",
       " 'still': 185875,\n",
       " 'relatively': 168987,\n",
       " 'there': 204279,\n",
       " 'no': 144389,\n",
       " 'really': 167534,\n",
       " 'cast': 59090,\n",
       " 'several': 176885,\n",
       " 'faces': 81066,\n",
       " 'will': 230407,\n",
       " 'familiar': 81645,\n",
       " 'entire': 77496,\n",
       " 'an': 33100,\n",
       " 'excellent': 79815,\n",
       " 'job': 120769,\n",
       " 'with': 231200,\n",
       " '\\n \\n ': 0,\n",
       " 'but': 55233,\n",
       " 'watch': 224142,\n",
       " 'because': 48827,\n",
       " 'end': 76351,\n",
       " 'situation': 180264,\n",
       " 'like': 126924,\n",
       " 'one': 153423,\n",
       " 'presented': 163577,\n",
       " 'now': 146760,\n",
       " 'blame': 52362,\n",
       " 'british': 54359,\n",
       " 'for': 87111,\n",
       " 'setting': 176810,\n",
       " 'hindus': 103492,\n",
       " 'muslims': 140422,\n",
       " 'against': 29872,\n",
       " 'each': 74800,\n",
       " 'other': 156127,\n",
       " 'then': 203901,\n",
       " 'them': 203472,\n",
       " 'into': 113706,\n",
       " 'two': 216877,\n",
       " 'countries': 66021,\n",
       " 'some': 182093,\n",
       " 'merit': 134952,\n",
       " 'view': 220903,\n",
       " \"'s\": 4219,\n",
       " 'also': 31802,\n",
       " 'true': 215952,\n",
       " 'that': 191543,\n",
       " 'forced': 88485,\n",
       " 'region': 168815,\n",
       " 'as': 42811,\n",
       " 'they': 204967,\n",
       " 'around': 42453,\n",
       " 'time': 209711,\n",
       " 'partition': 158823,\n",
       " 'seems': 175485,\n",
       " 'more': 136861,\n",
       " 'likely': 127658,\n",
       " 'simply': 179677,\n",
       " 'saw': 172810,\n",
       " 'between': 51345,\n",
       " 'were': 226152,\n",
       " 'clever': 62399,\n",
       " 'enough': 77149,\n",
       " 'exploit': 80571,\n",
       " 'their': 202859,\n",
       " 'own': 157786,\n",
       " 'ends': 76709,\n",
       " 'result': 169941,\n",
       " 'much': 139743,\n",
       " 'cruelty': 67253,\n",
       " 'inhumanity': 112841,\n",
       " 'very': 220028,\n",
       " 'unpleasant': 218279,\n",
       " 'remember': 169309,\n",
       " 'see': 174840,\n",
       " 'screen': 174101,\n",
       " 'never': 143379,\n",
       " 'painted': 158250,\n",
       " 'black': 52211,\n",
       " 'white': 228967,\n",
       " 'case': 58985,\n",
       " 'both': 53392,\n",
       " 'sides': 179401,\n",
       " 'hope': 105534,\n",
       " 'change': 59933,\n",
       " 'younger': 259694,\n",
       " 'generation': 91777,\n",
       " 'redemption': 168609,\n",
       " 'sort': 183622,\n",
       " 'make': 131252,\n",
       " 'choice': 61352,\n",
       " 'man': 131972,\n",
       " 'ruined': 171670,\n",
       " 'life': 126539,\n",
       " 'truly': 216095,\n",
       " 'loved': 130104,\n",
       " 'family': 81676,\n",
       " 'later': 124790,\n",
       " 'come': 63112,\n",
       " 'looking': 129249,\n",
       " 'point': 162209,\n",
       " 'without': 232583,\n",
       " 'great': 95376,\n",
       " 'pain': 158154,\n",
       " 'carries': 58869,\n",
       " 'message': 135015,\n",
       " 'have': 98847,\n",
       " 'grave': 95342,\n",
       " 'can': 57902,\n",
       " 'caring': 58782,\n",
       " 'people': 159402,\n",
       " 'reality': 167435,\n",
       " 'wrenching': 234830,\n",
       " 'since': 179847,\n",
       " 'real': 167146,\n",
       " 'across': 27577,\n",
       " 'india': 112505,\n",
       " '/': 18693,\n",
       " 'pakistan': 158299,\n",
       " 'border': 53152,\n",
       " 'sense': 176005,\n",
       " 'similar': 179551,\n",
       " '\"': 885,\n",
       " 'mr': 139656,\n",
       " '&': 2770,\n",
       " 'we': 225032,\n",
       " 'glad': 93403,\n",
       " 'seen': 175612,\n",
       " 'resolution': 169789,\n",
       " 'if': 109113,\n",
       " 'xxup': 256448,\n",
       " 'uk': 217458,\n",
       " 'us': 219019,\n",
       " 'could': 65644,\n",
       " 'deal': 68618,\n",
       " 'racism': 166252,\n",
       " 'would': 234332,\n",
       " 'certainly': 59737,\n",
       " 'better': 51101,\n",
       " 'off': 151385,\n",
       " 'xxmaj this': 245994,\n",
       " 'this is': 207344,\n",
       " 'is a': 114476,\n",
       " 'a extremely': 22470,\n",
       " 'extremely well': 80833,\n",
       " 'well -': 225791,\n",
       " '- made': 15557,\n",
       " 'made film': 130775,\n",
       " 'film .': 83997,\n",
       " 'xxmaj the': 245427,\n",
       " 'the acting': 194076,\n",
       " 'acting ,': 27710,\n",
       " ', script': 12382,\n",
       " 'script and': 174330,\n",
       " 'and camera': 34633,\n",
       " 'camera -': 57749,\n",
       " '- work': 16333,\n",
       " 'work are': 233548,\n",
       " 'are all': 41108,\n",
       " 'all first': 30719,\n",
       " 'first -': 85933,\n",
       " '- rate': 15858,\n",
       " 'rate .': 166640,\n",
       " 'the music': 198575,\n",
       " 'music is': 140331,\n",
       " 'is good': 115473,\n",
       " 'good ,': 94186,\n",
       " ', too': 13460,\n",
       " 'too ,': 214440,\n",
       " ', though': 13373,\n",
       " 'though it': 208734,\n",
       " 'it is': 118593,\n",
       " 'is mostly': 115911,\n",
       " 'mostly early': 137897,\n",
       " 'early in': 75040,\n",
       " 'in the': 111429,\n",
       " 'the film': 196399,\n",
       " 'film ,': 83916,\n",
       " ', when': 13766,\n",
       " 'when things': 227807,\n",
       " 'things are': 206134,\n",
       " 'are still': 42003,\n",
       " 'still relatively': 186064,\n",
       " 'relatively xxunk': 168998,\n",
       " 'xxmaj there': 245886,\n",
       " 'there are': 204362,\n",
       " 'are no': 41703,\n",
       " 'no really': 144777,\n",
       " 'really xxunk': 168073,\n",
       " 'xxunk in': 251885,\n",
       " 'the cast': 194895,\n",
       " 'cast ,': 59099,\n",
       " 'though several': 208754,\n",
       " 'several faces': 176894,\n",
       " 'faces will': 81082,\n",
       " 'will be': 230442,\n",
       " 'be familiar': 47794,\n",
       " 'familiar .': 81649,\n",
       " 'the entire': 196079,\n",
       " 'entire cast': 77501,\n",
       " 'cast does': 59149,\n",
       " 'does an': 72668,\n",
       " 'an excellent': 33479,\n",
       " 'excellent job': 79860,\n",
       " 'job with': 120843,\n",
       " 'with the': 232107,\n",
       " 'the script': 200200,\n",
       " 'script .': 174322,\n",
       " '. \\n \\n ': 16846,\n",
       " '\\n \\n  xxmaj': 201,\n",
       " 'xxmaj but': 237156,\n",
       " 'but it': 55712,\n",
       " 'is hard': 115506,\n",
       " 'to watch': 213697,\n",
       " 'watch ,': 224148,\n",
       " ', because': 9683,\n",
       " 'because there': 48997,\n",
       " 'there is': 204470,\n",
       " 'is no': 116027,\n",
       " 'no good': 144577,\n",
       " 'good end': 94348,\n",
       " 'end to': 76492,\n",
       " 'to a': 210460,\n",
       " 'a situation': 25006,\n",
       " 'situation like': 180285,\n",
       " 'like the': 127344,\n",
       " 'the one': 198859,\n",
       " 'one presented': 153923,\n",
       " 'presented .': 163580,\n",
       " 'xxmaj it': 240758,\n",
       " 'is now': 116143,\n",
       " 'now xxunk': 146943,\n",
       " 'xxunk to': 254882,\n",
       " 'to blame': 210953,\n",
       " 'blame the': 52371,\n",
       " 'the xxmaj': 201976,\n",
       " 'xxmaj british': 237029,\n",
       " 'british for': 54380,\n",
       " 'for setting': 87910,\n",
       " 'setting xxmaj': 176836,\n",
       " 'xxmaj hindus': 240216,\n",
       " 'hindus and': 103493,\n",
       " 'and xxmaj': 38626,\n",
       " 'xxmaj muslims': 242539,\n",
       " 'muslims against': 140426,\n",
       " 'against each': 29885,\n",
       " 'each other': 74866,\n",
       " 'other ,': 156138,\n",
       " ', and': 9218,\n",
       " 'and then': 37939,\n",
       " 'then xxunk': 204237,\n",
       " 'xxunk xxunk': 256140,\n",
       " 'xxunk them': 254721,\n",
       " 'them into': 203630,\n",
       " 'into two': 114027,\n",
       " 'two countries': 216949,\n",
       " 'countries .': 66022,\n",
       " 'is some': 116671,\n",
       " 'some merit': 182403,\n",
       " 'merit in': 134955,\n",
       " 'this view': 208179,\n",
       " 'view ,': 220908,\n",
       " ', but': 9824,\n",
       " \"it 's\": 117610,\n",
       " \"'s also\": 4429,\n",
       " 'also true': 32196,\n",
       " 'true that': 216052,\n",
       " 'that no': 192798,\n",
       " 'no one': 144700,\n",
       " 'one forced': 153682,\n",
       " 'forced xxmaj': 88517,\n",
       " 'muslims in': 140430,\n",
       " 'the region': 199789,\n",
       " 'region to': 168820,\n",
       " 'to xxunk': 213936,\n",
       " 'xxunk each': 250866,\n",
       " 'other as': 156178,\n",
       " 'as they': 43790,\n",
       " 'they did': 205251,\n",
       " 'did around': 70522,\n",
       " 'around the': 42557,\n",
       " 'the time': 201168,\n",
       " 'time of': 209939,\n",
       " 'of partition': 149450,\n",
       " 'partition .': 158824,\n",
       " 'it seems': 119112,\n",
       " 'seems more': 175532,\n",
       " 'more likely': 137171,\n",
       " 'likely that': 127663,\n",
       " 'that the': 193183,\n",
       " 'british simply': 54388,\n",
       " 'simply saw': 179788,\n",
       " 'saw the': 172867,\n",
       " 'the xxunk': 202357,\n",
       " 'xxunk between': 250039,\n",
       " 'between the': 51422,\n",
       " 'xxunk and': 249319,\n",
       " 'and were': 38456,\n",
       " 'were clever': 226268,\n",
       " 'clever enough': 62419,\n",
       " 'enough to': 77250,\n",
       " 'to exploit': 211477,\n",
       " 'exploit them': 80574,\n",
       " 'them to': 203708,\n",
       " 'to their': 213505,\n",
       " 'their own': 203225,\n",
       " 'own ends': 157849,\n",
       " 'ends .': 76714,\n",
       " 'the result': 199865,\n",
       " 'result is': 169950,\n",
       " 'is that': 116818,\n",
       " 'that there': 193302,\n",
       " 'is much': 115922,\n",
       " 'much cruelty': 139835,\n",
       " 'cruelty and': 67256,\n",
       " 'and inhumanity': 36008,\n",
       " 'inhumanity in': 112844,\n",
       " 'the situation': 200490,\n",
       " 'situation and': 180273,\n",
       " 'and this': 38049,\n",
       " 'is very': 117159,\n",
       " 'very unpleasant': 220539,\n",
       " 'unpleasant to': 218286,\n",
       " 'to remember': 212726,\n",
       " 'remember and': 169322,\n",
       " 'and to': 38124,\n",
       " 'to see': 212884,\n",
       " 'see on': 175066,\n",
       " 'on the': 152927,\n",
       " 'the screen': 200165,\n",
       " 'screen .': 174117,\n",
       " 'is never': 116013,\n",
       " 'never painted': 143594,\n",
       " 'painted as': 158255,\n",
       " 'as a': 42836,\n",
       " 'a black': 21636,\n",
       " 'black -': 52220,\n",
       " '- and': 14769,\n",
       " 'and -': 34009,\n",
       " '- white': 16300,\n",
       " 'white case': 228982,\n",
       " 'case .': 59002,\n",
       " 'is xxunk': 117387,\n",
       " 'and xxunk': 38901,\n",
       " 'xxunk on': 253106,\n",
       " 'on both': 152543,\n",
       " 'both sides': 53519,\n",
       " 'sides ,': 179402,\n",
       " 'and also': 34261,\n",
       " 'also the': 32167,\n",
       " 'the hope': 197270,\n",
       " 'hope for': 105552,\n",
       " 'for change': 87380,\n",
       " 'change in': 59950,\n",
       " 'the younger': 202778,\n",
       " 'younger generation': 259701,\n",
       " 'generation .': 91783,\n",
       " 'is redemption': 116438,\n",
       " 'redemption of': 168616,\n",
       " 'of a': 147636,\n",
       " 'a sort': 25109,\n",
       " 'sort ,': 183623,\n",
       " ', in': 11128,\n",
       " 'the end': 196001,\n",
       " 'end ,': 76358,\n",
       " 'when xxmaj': 227831,\n",
       " 'xxunk has': 251545,\n",
       " 'has to': 98656,\n",
       " 'to make': 212219,\n",
       " 'make a': 131263,\n",
       " 'a hard': 23147,\n",
       " 'hard choice': 97903,\n",
       " 'choice between': 61360,\n",
       " 'between a': 51358,\n",
       " 'a man': 23695,\n",
       " 'man who': 132176,\n",
       " 'who has': 229355,\n",
       " 'has ruined': 98579,\n",
       " 'ruined her': 171678,\n",
       " 'her life': 102082,\n",
       " 'life ,': 126556,\n",
       " 'but also': 55315,\n",
       " 'also truly': 32198,\n",
       " 'truly loved': 216140,\n",
       " 'loved her': 130113,\n",
       " 'her ,': 101714,\n",
       " 'and her': 35725,\n",
       " 'her family': 101954,\n",
       " 'family which': 81823,\n",
       " 'which has': 228349,\n",
       " 'has xxunk': 98702,\n",
       " 'xxunk her': 251616,\n",
       " ', then': 13182,\n",
       " 'then later': 204101,\n",
       " 'later come': 124832,\n",
       " 'come looking': 63173,\n",
       " 'looking for': 129282,\n",
       " 'for her': 87517,\n",
       " 'her .': 101732,\n",
       " 'but by': 55387,\n",
       " 'by that': 56989,\n",
       " 'that point': 192920,\n",
       " 'point ,': 162212,\n",
       " ', she': 12453,\n",
       " 'she has': 177530,\n",
       " 'has no': 98504,\n",
       " 'no xxunk': 144906,\n",
       " 'xxunk that': 254401,\n",
       " 'that is': 192462,\n",
       " 'is without': 117292,\n",
       " 'without great': 232660,\n",
       " 'great pain': 95613,\n",
       " 'pain for': 158170,\n",
       " 'this film': 207054,\n",
       " 'film carries': 84164,\n",
       " 'carries the': 58873,\n",
       " 'the message': 198187,\n",
       " 'message that': 135052,\n",
       " 'that both': 191889,\n",
       " 'both xxmaj': 53555,\n",
       " 'muslims and': 140428,\n",
       " 'hindus have': 103495,\n",
       " 'have their': 99579,\n",
       " 'their grave': 203092,\n",
       " 'grave xxunk': 95357,\n",
       " 'xxunk ,': 248429,\n",
       " 'also that': 32164,\n",
       " 'both can': 53423,\n",
       " 'can be': 57953,\n",
       " 'be xxunk': 48432,\n",
       " 'and caring': 34653,\n",
       " 'caring people': 58792,\n",
       " 'people .': 159438,\n",
       " 'the reality': 199735,\n",
       " 'reality of': 167462,\n",
       " 'partition makes': 158828,\n",
       " 'makes that': 131687,\n",
       " 'that xxunk': 193725,\n",
       " 'xxunk all': 249241,\n",
       " 'all the': 31003,\n",
       " 'the more': 198292,\n",
       " 'more wrenching': 137414,\n",
       " 'wrenching ,': 234831,\n",
       " ', since': 12536,\n",
       " 'since there': 179925,\n",
       " 'there can': 204435,\n",
       " 'can never': 58123,\n",
       " 'never be': 143401,\n",
       " 'be real': 48117,\n",
       " 'real xxunk': 167347,\n",
       " 'xxunk across': 249146,\n",
       " 'across the': 27597,\n",
       " 'xxmaj india': 240621,\n",
       " 'india /': 112512,\n",
       " '/ xxmaj': 18915,\n",
       " 'xxmaj pakistan': 243213,\n",
       " 'pakistan border': 158302,\n",
       " 'border .': 53155,\n",
       " 'xxmaj in': 240564,\n",
       " 'in that': 111398,\n",
       " 'that sense': 193043,\n",
       " 'sense ,': 176011,\n",
       " ', it': 11295,\n",
       " 'is similar': 116600,\n",
       " 'similar to': 179583,\n",
       " 'to \"': 210398,\n",
       " '\" xxmaj': 2329,\n",
       " 'xxmaj mr': 242491,\n",
       " 'mr &': 139657,\n",
       " '& xxmaj': 2909,\n",
       " 'xxunk xxmaj': 255692,\n",
       " 'xxunk \"': 248003,\n",
       " '\" .': 1004,\n",
       " ', we': 13687,\n",
       " 'we were': 225359,\n",
       " 'were glad': 226367,\n",
       " 'glad to': 93416,\n",
       " 'to have': 211763,\n",
       " 'have seen': 99454,\n",
       " 'seen the': 175800,\n",
       " ', even': 10434,\n",
       " 'even though': 78686,\n",
       " 'though the': 208765,\n",
       " 'the resolution': 199849,\n",
       " 'resolution was': 169805,\n",
       " 'was xxunk': 223972,\n",
       " 'xxmaj if': 240505,\n",
       " 'if the': 109311,\n",
       " 'the xxup': 202682,\n",
       " 'xxup uk': 257624,\n",
       " 'uk and': 217465,\n",
       " 'and xxup': 39060,\n",
       " 'xxup us': 257637,\n",
       " 'us could': 219071,\n",
       " 'could deal': 65730,\n",
       " 'deal with': 68637,\n",
       " 'with their': 232242,\n",
       " 'own xxunk': 157977,\n",
       " 'xxunk of': 252919,\n",
       " 'of racism': 149579,\n",
       " 'racism with': 166272,\n",
       " 'with this': 232274,\n",
       " 'this kind': 207446,\n",
       " 'of xxunk': 151201,\n",
       " ', they': 13241,\n",
       " 'they would': 205841,\n",
       " 'would certainly': 234439,\n",
       " 'certainly be': 59742,\n",
       " 'be better': 47645,\n",
       " 'better off': 51236,\n",
       " 'off .': 151405,\n",
       " 'xxbos xxmaj this': 235568,\n",
       " 'xxmaj this is': 246034,\n",
       " 'this is a': 207348,\n",
       " 'is a extremely': 114525,\n",
       " 'a extremely well': 22471,\n",
       " 'extremely well -': 80834,\n",
       " 'well - made': 225801,\n",
       " '- made film': 15559,\n",
       " 'made film .': 130777,\n",
       " 'film . xxmaj': 84004,\n",
       " '. xxmaj the': 18078,\n",
       " 'xxmaj the acting': 245437,\n",
       " 'the acting ,': 194078,\n",
       " 'acting , script': 27731,\n",
       " ', script and': 12383,\n",
       " 'script and camera': 174332,\n",
       " 'and camera -': 34634,\n",
       " 'camera - work': 57750,\n",
       " '- work are': 16335,\n",
       " 'work are all': 233549,\n",
       " 'are all first': 41114,\n",
       " 'all first -': 30720,\n",
       " 'first - rate': 85936,\n",
       " '- rate .': 15859,\n",
       " 'rate . xxmaj': 166641,\n",
       " 'xxmaj the music': 245666,\n",
       " 'the music is': 198584,\n",
       " 'music is good': 140334,\n",
       " 'is good ,': 115476,\n",
       " 'good , too': 94203,\n",
       " ', too ,': 13462,\n",
       " 'too , though': 214451,\n",
       " ', though it': 13387,\n",
       " 'though it is': 208738,\n",
       " 'it is mostly': 118663,\n",
       " 'is mostly early': 115916,\n",
       " 'mostly early in': 137898,\n",
       " 'early in the': 75041,\n",
       " 'in the film': 111547,\n",
       " 'the film ,': 196405,\n",
       " 'film , when': 83969,\n",
       " ', when things': 13787,\n",
       " 'when things are': 227808,\n",
       " 'things are still': 206138,\n",
       " 'are still relatively': 42006,\n",
       " 'still relatively xxunk': 186065,\n",
       " 'relatively xxunk .': 168999,\n",
       " '. xxmaj there': 18081,\n",
       " 'xxmaj there are': 245891,\n",
       " 'there are no': 204396,\n",
       " 'are no really': 41714,\n",
       " 'no really xxunk': 144778,\n",
       " 'really xxunk in': 168079,\n",
       " 'xxunk in the': 251957,\n",
       " 'in the cast': 111486,\n",
       " 'the cast ,': 194896,\n",
       " 'cast , though': 59111,\n",
       " ', though several': 13392,\n",
       " 'though several faces': 208755,\n",
       " 'several faces will': 176895,\n",
       " 'faces will be': 81083,\n",
       " 'will be familiar': 230458,\n",
       " 'be familiar .': 47795,\n",
       " 'familiar . xxmaj': 81650,\n",
       " 'xxmaj the entire': 245540,\n",
       " 'the entire cast': 196082,\n",
       " 'entire cast does': 77503,\n",
       " 'cast does an': 59150,\n",
       " 'does an excellent': 72669,\n",
       " 'an excellent job': 33484,\n",
       " 'excellent job with': 79863,\n",
       " 'job with the': 120844,\n",
       " 'with the script': 232219,\n",
       " 'the script .': 200204,\n",
       " 'script . \\n \\n ': 174323,\n",
       " '. \\n \\n  xxmaj': 16880,\n",
       " '\\n \\n  xxmaj but': 256,\n",
       " 'xxmaj but it': 237188,\n",
       " 'but it is': 55729,\n",
       " 'it is hard': 118647,\n",
       " 'is hard to': 115508,\n",
       " 'hard to watch': 97958,\n",
       " 'to watch ,': 213700,\n",
       " 'watch , because': 224149,\n",
       " ', because there': 9696,\n",
       " 'because there is': 49000,\n",
       " 'there is no': 204496,\n",
       " 'is no good': 116036,\n",
       " 'no good end': 144580,\n",
       " 'good end to': 94350,\n",
       " 'end to a': 76493,\n",
       " 'to a situation': 210514,\n",
       " 'a situation like': 25008,\n",
       " 'situation like the': 180286,\n",
       " 'like the one': 127370,\n",
       " 'the one presented': 198874,\n",
       " 'one presented .': 153924,\n",
       " 'presented . xxmaj': 163581,\n",
       " '. xxmaj it': 17723,\n",
       " 'xxmaj it is': 240800,\n",
       " 'it is now': 118672,\n",
       " 'is now xxunk': 116149,\n",
       " 'now xxunk to': 146947,\n",
       " 'xxunk to blame': 254909,\n",
       " 'to blame the': 210955,\n",
       " 'blame the xxmaj': 52374,\n",
       " 'the xxmaj british': 202012,\n",
       " 'xxmaj british for': 237039,\n",
       " 'british for setting': 54381,\n",
       " 'for setting xxmaj': 87911,\n",
       " 'setting xxmaj hindus': 176837,\n",
       " 'xxmaj hindus and': 240217,\n",
       " 'hindus and xxmaj': 103494,\n",
       " 'and xxmaj muslims': 38809,\n",
       " 'xxmaj muslims against': 242541,\n",
       " 'muslims against each': 140427,\n",
       " 'against each other': 29886,\n",
       " 'each other ,': 74868,\n",
       " 'other , and': 156139,\n",
       " ', and then': 9449,\n",
       " 'and then xxunk': 37980,\n",
       " 'then xxunk xxunk': 204248,\n",
       " 'xxunk xxunk them': 256288,\n",
       " 'xxunk them into': 254727,\n",
       " 'them into two': 203633,\n",
       " 'into two countries': 114028,\n",
       " 'two countries .': 216950,\n",
       " 'countries . xxmaj': 66023,\n",
       " 'xxmaj there is': 245894,\n",
       " 'there is some': 204507,\n",
       " 'is some merit': 116675,\n",
       " 'some merit in': 182404,\n",
       " 'merit in this': 134956,\n",
       " 'in this view': 111857,\n",
       " 'this view ,': 208180,\n",
       " 'view , but': 220910,\n",
       " ', but it': 9898,\n",
       " \"but it 's\": 55713,\n",
       " \"it 's also\": 117622,\n",
       " \"'s also true\": 4437,\n",
       " 'also true that': 32197,\n",
       " 'true that no': 216053,\n",
       " 'that no one': 192803,\n",
       " 'no one forced': 144708,\n",
       " 'one forced xxmaj': 153683,\n",
       " 'forced xxmaj hindus': 88518,\n",
       " 'xxmaj muslims in': 242543,\n",
       " 'muslims in the': 140431,\n",
       " 'in the region': 111656,\n",
       " 'the region to': 199790,\n",
       " 'region to xxunk': 168821,\n",
       " 'to xxunk each': 213967,\n",
       " 'xxunk each other': 250870,\n",
       " 'each other as': 74875,\n",
       " 'other as they': 156179,\n",
       " 'as they did': 43796,\n",
       " 'they did around': 205254,\n",
       " 'did around the': 70523,\n",
       " 'around the time': 42568,\n",
       " 'the time of': 201180,\n",
       " 'time of partition': 209941,\n",
       " 'of partition .': 149451,\n",
       " 'partition . xxmaj': 158825,\n",
       " 'xxmaj it seems': 240832,\n",
       " 'it seems more': 119120,\n",
       " 'seems more likely': 175533,\n",
       " 'more likely that': 137173,\n",
       " 'likely that the': 127664,\n",
       " 'that the xxmaj': 193292,\n",
       " 'xxmaj british simply': 237043,\n",
       " 'british simply saw': 54389,\n",
       " 'simply saw the': 179789,\n",
       " 'saw the xxunk': 172876,\n",
       " 'the xxunk between': 202403,\n",
       " 'xxunk between the': 250050,\n",
       " 'between the xxunk': 51448,\n",
       " 'the xxunk and': 202387,\n",
       " 'xxunk and were': 249613,\n",
       " 'and were clever': 38457,\n",
       " 'were clever enough': 226269,\n",
       " 'clever enough to': 62420,\n",
       " 'enough to exploit': 77256,\n",
       " 'to exploit them': 211479,\n",
       " 'exploit them to': 80575,\n",
       " 'them to their': 203720,\n",
       " 'to their own': 213515,\n",
       " 'their own ends': 203232,\n",
       " 'own ends .': 157850,\n",
       " 'ends . \\n \\n ': 76715,\n",
       " '\\n \\n  xxmaj the': 500,\n",
       " 'xxmaj the result': 245734,\n",
       " 'the result is': 199866,\n",
       " 'result is that': 169954,\n",
       " 'is that there': 116849,\n",
       " 'that there is': 193305,\n",
       " 'there is much': 204492,\n",
       " 'is much cruelty': 115924,\n",
       " 'much cruelty and': 139836,\n",
       " 'cruelty and inhumanity': 67257,\n",
       " 'and inhumanity in': 36010,\n",
       " 'inhumanity in the': 112845,\n",
       " 'in the situation': 111689,\n",
       " 'the situation and': 200494,\n",
       " 'situation and this': 180274,\n",
       " 'and this is': 38058,\n",
       " 'this is very': 207419,\n",
       " 'is very unpleasant': 117194,\n",
       " 'very unpleasant to': 220540,\n",
       " 'unpleasant to remember': 218287,\n",
       " ...}"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ngram_doc = veczr.transform(valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200x260401 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 93549 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ngram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = veczr.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the room she',\n",
       " 'the room when',\n",
       " 'the room where',\n",
       " 'the rooms',\n",
       " 'the rooms are']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200000:200005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarized Naive Bayes, using ngrams from CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=movie_reviews.train.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is the inverse of regularization strength; smaller values specify stronger regularization.  Regularized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(train_ngram_doc.sign(), y.items);\n",
    "\n",
    "preds = m.predict(val_ngram_doc.sign())\n",
    "(preds.T==valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/racheltho/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(train_ngram_doc, y.items);\n",
    "\n",
    "preds = m.predict(val_ngram_doc)\n",
    "(preds.T==valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using my ngrams, binarized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = LogisticRegression(C=0.1, dual=True)\n",
    "m2.fit(trn_x_ngram_sgn, y.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = m2.predict(val_x_ngram_sgn)\n",
    "(preds.T==valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse performance when not binarized.  I manually tried several different C values, and this was the best I found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=50000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = LogisticRegression(C=0.0001, dual=True, max_iter=50000)\n",
    "m2.fit(train_ngram_doc_matrix, y.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = m2.predict(valid_ngram_doc_matrix)\n",
    "(preds.T==valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-count ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the $\\text{log-count ratio}$ `r`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_ngram_doc_matrix.sign()\n",
    "val_x=valid_ngram_doc_matrix.sign()\n",
    "y=movie_reviews.train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = y.c2i['positive']\n",
    "negative = y.c2i['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=260428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (y.items == positive)[:k]\n",
    "neg = (y.items == negative)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [o == positive for o in movie_reviews.valid.y.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
    "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1/pr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.994341, 1.088542, 1.      , 1.088542, ..., 0.544271, 0.544271, 0.544271, 0.544271])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the features are the trigrams' log-count ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = xx.multiply(r)\n",
    "m = LogisticRegression(dual=True, C=0.1)\n",
    "m.fit(x_nb, y.items);\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds.T==valid_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
